# Llama Stack Example Notebooks

This directory contains notebooks and example code for demonstrating various Llama Stack capabilities and use cases.

## Current Examples

### [01-responses/](./01-responses/)

An introduction to the Llama Stack Responses API, including:

- Simple model inference
- Retrieval-Augmented Generation (RAG)
- Model Context Protocol (MCP) tool calling
- Integration examples with the Llama Stack client, the OpenAI client, and LangChain

## Contributing New Notebooks

We welcome contributions of new example notebooks! If you've built something interesting with Llama Stack, please consider sharing it with the community.

### What Makes a Good Example Notebook?

- **Clear documentation**: Well-commented code with explanations
- **Complete setup**: Include all prerequisites and dependencies
- **Realistic use cases**: Practical applications that others can learn from
- **Clean structure**: Organized code that's easy to follow

### How to Contribute

1. Create a new directory following the naming pattern: `NN-topic-name/`
2. Add in a notebook with instructions, code, and explanations
3. Include a comprehensive README.md explaining your example
4. If needed, add additional support files to make your notebook work
5. Ensure your notebook runs from start to finish
6. Submit a pull request with your contribution

### Ideas for Future Notebooks

Some areas we'd love to see examples for:

- Advanced RAG implementations
- Multi-modal applications
- Custom tool development
- Production deployment patterns
- Performance optimization techniques
- Integration with other AI frameworks

## Getting Started

Each example directory contains its own README with specific instructions for getting started.

## Support

For questions about specific examples, check the README in each directory. For general Llama Stack questions, see the [official documentation](https://llama-stack.readthedocs.io/).
